{"cells":[{"cell_type":"markdown","metadata":{"id":"uXnpy4tSUHWu"},"source":["# **Step 1: Training the classifier**\n"]},{"cell_type":"markdown","metadata":{"id":"Xh_bTdsKaF7p"},"source":["Connect to Google Earth Engine"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eaXkk4BLUSkl"},"outputs":[],"source":["# import Google Earth Engine API\n","import ee\n","# Trigger the authentication flow.\n","ee.Authenticate()\n","# Initialize the library.\n","ee.Initialize(project='...') # specify your project"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1LO6aWo8RfcK"},"outputs":[],"source":["import geemap\n","from ee import batch\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import pandas as pd\n","import geopandas as gpd\n","import numpy as np\n","import math\n","import folium\n","import os\n","from math import ceil"]},{"cell_type":"markdown","metadata":{"id":"ZRj0Q_nm1F5a"},"source":["# **Import data**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IX8kUm5zFYSM"},"outputs":[],"source":["# Mountain areas to cut the global images - regions for upscaling\n","himalaya = ee.Geometry.Rectangle(62, 20, 110, 45)\n","alps = ee.Geometry.Rectangle(4, 43, 15, 48)\n","rockyMountains = ee.Geometry.Rectangle(-120, 30, -102, 48)\n","andes = ee.Geometry.Rectangle(-80, -40, -60, 11)\n","\n","MountainRegions = ee.FeatureCollection([himalaya, alps, rockyMountains, andes])\n","MountainRegions_mask = himalaya.union(alps).union(rockyMountains).union(andes)"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"d_vo85-pU16y"},"outputs":[],"source":["# import satellite data available in the google earth engine data catalog for the four major mountain regions\n","ecoregions_1 = ee.FeatureCollection(\"RESOLVE/ECOREGIONS/2017\") # RESOLVE dataset with 846 global ecoregions\n","dem = ee.Image(\"USGS/SRTMGL1_003\") # Nasa DEM 30m resolution"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SRAc5tCpnhw_"},"outputs":[],"source":["# Mountain areas to cut the global images and to use as regions for upscaling\n","himalaya_dem = dem.clip(himalaya)\n","himalaya_mask = himalaya_dem.gt(3500)\n","himalaya_dem_masked = himalaya_dem.updateMask(himalaya_mask)\n","\n","alps_dem = dem.clip(alps)\n","alps_mask = alps_dem.gt(1800)\n","alps_dem_masked = alps_dem.updateMask(alps_mask)\n","\n","rockyMountains_dem = dem.clip(rockyMountains)\n","rockyMountains_mask = rockyMountains_dem.gt(2000)\n","rockyMountains_dem_masked = rockyMountains_dem.updateMask(rockyMountains_mask)\n","\n","andes_dem = dem.clip(andes)\n","andes_mask = andes_dem.gt(3500)\n","andes_dem_masked = andes_dem.updateMask(andes_mask)\n","\n","# mask for all high mountain regions (DEM > 2000 masl)\n","dem_clipped = dem.clipToCollection(MountainRegions)\n","#mountain_mask = dem_clipped.gt(2000)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TmodwsQmsBmg"},"outputs":[],"source":["# cut the ecoregions to the mask\n","def clip_feature(feature):\n","    return feature.intersection(MountainRegions_mask, ee.ErrorMargin(5))\n","\n","ecoregions = ecoregions_1.map(clip_feature) #clips to exact extend of MountainRegion mask - use for upscaling."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vWHlaIHxYsCq"},"outputs":[],"source":["# clip all DEM derived input images to mountain regions\n","elevation_temp = dem_clipped.select('elevation') # elevation clipped to MountainRegions extent\n","slope_temp = ee.Terrain.slope(dem_clipped) # slope clipped to MountainRegions extent"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"whHO1n-oYsXe"},"outputs":[],"source":["# high mountain wetland training areas\n","hw_alps_west = ee.FeatureCollection('projects/.../assets/hw_alps_west')\n","hw_alps_center = ee.FeatureCollection('projects/.../assets/hw_alps_center')\n","hw_alps_east = ee.FeatureCollection('projects/.../assets/hw_alps_east')\n","hw_andes_eastCR = ee.FeatureCollection('projects/.../hw_andes_eastCR')\n","hw_andes_paramo = ee.FeatureCollection('projects/.../assets/hw_andes_paramo')\n","hw_andes_yungas = ee.FeatureCollection('projects/.../assets/hw_andes_yungas')\n","hw_andes_wet_puna = ee.FeatureCollection('projects/.../assets/hw_andes_wet_puna')\n","hw_andes_puna = ee.FeatureCollection('projects/.../assets/hw_andes_puna')\n","hw_rockies_colorado = ee.FeatureCollection('projects/.../assets/hw_rockies_colorado_simplified')\n","hw_rockies_wyoming = ee.FeatureCollection('projects/.../assets/hw_rockies_wyoming_simplified')\n","hw_highAsia_tibet_plateau = ee.FeatureCollection('projects/.../assets/hw_ha_tibet_plateau_shrublands')\n","hw_highAsia_east_himalaya = ee.FeatureCollection('projects/.../assets/hw_ha_east_him_alpine')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wnQ8qP9OYg6J"},"outputs":[],"source":["hw_list = [\n","    hw_alps_west, hw_alps_center, hw_alps_east, hw_andes_eastCR, hw_andes_paramo, hw_andes_yungas, hw_andes_wet_puna,\n","    hw_andes_puna, hw_rockies_colorado, hw_rockies_wyoming, hw_highAsia_tibet_plateau, hw_highAsia_east_himalaya\n","]\n","\n","hws = ee.FeatureCollection(hw_list[0])\n","for hw in hw_list[1:]:\n","    hws = hws.merge(hw)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eS76bZCPZchL"},"outputs":[],"source":["# non-wetland training areas\n","nhw_alps_west = ee.FeatureCollection('projects/.../assets/nhw_alps_west')\n","nhw_alps_center = ee.FeatureCollection('projects/.../assets/nhw_alps_center')\n","nhw_alps_east = ee.FeatureCollection('projects/.../assets/nhw_alps_east')\n","nhw_andes_eastCR = ee.FeatureCollection('projects/.../assets/nhw_andes_eastCR')\n","nhw_andes_paramo = ee.FeatureCollection('projects/.../assets/nhw_andes_paramo')\n","nhw_andes_yungas = ee.FeatureCollection('projects/.../assets/nhw_andes_yungas')\n","nhw_andes_wet_puna = ee.FeatureCollection('projects/.../assets/nhw_andes_wet_puna')\n","nhw_andes_puna = ee.FeatureCollection('projects/.../assets/nhw_andes_puna')\n","nhw_rockies_colorado = ee.FeatureCollection('projects/.../assets/nhw_rockies_colorado_simplified')\n","nhw_rockies_wyoming = ee.FeatureCollection('projects/.../assets/nhw_rockies_wyoming_simplified')\n","nhw_highAsia_tibet_plateau = ee.FeatureCollection('projects/.../assets/nhw_ha_tibet_plateau_shrublands')\n","nhw_highAsia_east_himalaya = ee.FeatureCollection('projects/.../assets/nhw_ha_east_him_alpine')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Hupup2IeZvee"},"outputs":[],"source":["nhw_list = [\n","    nhw_alps_west, nhw_alps_center, nhw_alps_east, nhw_andes_eastCR, nhw_andes_paramo, nhw_andes_yungas, nhw_andes_wet_puna,\n","    nhw_andes_puna, nhw_rockies_colorado, nhw_rockies_wyoming, nhw_highAsia_tibet_plateau, nhw_highAsia_east_himalaya\n","]\n","\n","nhws = ee.FeatureCollection(nhw_list[0])\n","for nhw in nhw_list[1:]:\n","    nhws = nhws.merge(nhw)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"InGRtGJngHbm"},"outputs":[],"source":["# create training region from all regions with training data\n","training_alps_west = ee.FeatureCollection('projects/.../assets/Training_alps_west')\n","training_alps_center = ee.FeatureCollection('projects/.../assets/Training_alps_center')\n","training_alps_east = ee.FeatureCollection('projects/.../assets/Training_alps_east')\n","training_andes_eastCR = ee.FeatureCollection('projects/.../assets/Training_andes_eastCR')\n","training_andes_paramo = ee.FeatureCollection('projects/.../assets/Training_andes_paramo')\n","training_andes_yungas = ee.FeatureCollection('projects/.../assets/Training_andes_yungas')\n","training_andes_wet_puna = ee.FeatureCollection('projects/.../assets/Training_andes_wet_puna')\n","training_andes_puna = ee.FeatureCollection('projects/.../assets/Training_andes_puna')\n","training_rockies_colorado = ee.FeatureCollection('projects/.../assets/Training_rockies_colorado')\n","training_rockies_wyoming = ee.FeatureCollection('projects/.../assets/Training_rockies_wyoming')\n","training_highAsia_tibet_plateau = ee.FeatureCollection('projects/.../assets/Training_ha_tibet_plateau_shrublands')\n","training_highAsia_east_himalaya = ee.FeatureCollection('projects/.../assets/Training_ha_east_him_alpine')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pb5XKQFiSKxY"},"outputs":[],"source":["training_list = [\n","    training_alps_west, training_alps_center, training_alps_east, training_andes_eastCR, training_andes_paramo, training_andes_yungas,\n","    training_andes_wet_puna, training_andes_puna, training_rockies_colorado, training_rockies_wyoming, training_highAsia_tibet_plateau, training_highAsia_east_himalaya\n","]\n","\n","all_training_regions = ee.FeatureCollection(training_list[0])\n","for roi in training_list[1:]:\n","    all_training_regions = all_training_regions.merge(roi)"]},{"cell_type":"markdown","metadata":{"id":"mSy3gunv1r-h"},"source":["# **Define functions**"]},{"cell_type":"markdown","metadata":{"id":"EeQ-yTK06rM_"},"source":["Cloud masking"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SgOnOX166tZk"},"outputs":[],"source":["# function for cloud masking\n","def cloudless(image):\n","  qa = image.select('QA60')\n","  cloudBitMask = 1 << 10\n","  cirrusBitMask = 1 << 11\n","  mask_clouds = qa.bitwiseAnd(cloudBitMask).eq(0).And(\n","      qa.bitwiseAnd(cirrusBitMask).eq(0))\n","  return image.updateMask(mask_clouds).divide(10000)#.clip(training_region) #change to '.clip(all_training_regions)' if run for all mountain regions\n","\n","  # the division by 10,000 is done because Sentinel-2 stores reflectance values as integers scaled by a factor of 10,000.\n","  # By dividing the pixel values by 10,000, we convert the data back to its original reflectance range (0 to 1)."]},{"cell_type":"markdown","metadata":{"id":"P7v2EEBt4Yft"},"source":["Function to create indices"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QmjzZGVN4XHW"},"outputs":[],"source":["# function that computes all vegetation indices and adds them as a band to the image collection\n","def spectral(image):\n","  ndvi = image.normalizedDifference(['B8', 'B4']).rename('ndvi')\n","  ndwi = image.normalizedDifference(['B3', 'B8']).rename('ndwi')\n","\n","  # TCG\n","  tcg = image.expression(\n","    '-0.2941*BLUE - 0.243*GREEN - 0.5424*RED + 0.7276*NIR + 0.0713*SWIRI - 0.1608*SWIRII',{\n","      'BLUE': image.select('B2'),\n","      'GREEN': image.select('B3'),\n","      'RED': image.select('B4'),\n","      'NIR': image.select('B8'),\n","      'SWIRI': image.select('B11'),\n","      'SWIRII': image.select('B12'),\n","    }).rename('tcg')\n","\n","  # ARI\n","  ari = image.expression(\n","    '(B8 / B2) - (B8 / B3)', {\n","        'B8': image.select(['B8']),\n","        'B2': image.select(['B2']),\n","        'B3': image.select(['B3']),\n","      }).rename('ari')\n","\n","  # PSRI\n","  psri = image.expression(\n","    '(B4 - B2) / B5', {\n","      'B4': image.select(['B4']),\n","      'B2': image.select(['B2']),\n","      'B5': image.select(['B5']),\n","      }).rename('psri')\n","\n","\n","  # REIP\n","  reip = image.expression(\n","    '702 + 40*((((RED + RE3)/2) - RE1) / (RE2 - RE1))', {\n","      'RE1': image.select(['B5']),\n","      'RE2': image.select(['B6']),\n","      'RE3': image.select(['B7']),\n","      'RED': image.select(['B4']),\n","      }).rename('reip')\n","\n","  # add indices to image collection\n","  indices = image.addBands(ndvi).addBands(ndwi).addBands(tcg).addBands(ari).addBands(reip).addBands(psri)\n","  return indices"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KOjLCL7Izu4A"},"outputs":[],"source":["def clip_image(image):\n","    return image.clipToCollection(mask)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"klFkrriC3-sn"},"outputs":[],"source":["# mask for angle selection of sentinel-1 data. Selecting images taken with angles between 30-45 degrees.\n","def mask_ang_gt_30(image):\n","    ang = image.select(['angle'])\n","    return image.updateMask(ang.gt(30))\n","\n","def mask_ang_lt_45(image):\n","    ang = image.select(['angle'])\n","    return image.updateMask(ang.lt(45))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iK1PBmmt4cap"},"outputs":[],"source":["# Function to calculate gamma0 and NDPI\n","def calculate_gamma0_and_ndpi(image):\n","    # Get the incidence angle in radians\n","    angle = image.select('angle')\n","    angle_rad = angle.multiply(math.pi / 180)\n","\n","    # Calculate gamma0 for VV and VH\n","    gamma0_vv = image.select('VV').divide(angle_rad.cos()).rename('gamma0_VV')\n","    gamma0_vh = image.select('VH').divide(angle_rad.cos()).rename('gamma0_VH')\n","\n","    # Calculate NDPI\n","    ndpi = gamma0_vv.subtract(gamma0_vh).divide(gamma0_vv.add(gamma0_vh)).rename('NDPI')\n","\n","    return image.addBands([gamma0_vv, gamma0_vh, ndpi])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZaICuDgn4mV2"},"outputs":[],"source":["# Define the function to apply the convolution filter.\n","# Define the 3x3 boxcar kernel.\n","# used to create a circular kernel with a specified radius, unit, and normalization\n","boxcar = ee.Kernel.circle(radius=3, units='pixels', normalize=True)\n","\n","def apply_filter(image):\n","    filtered_image = image.convolve(boxcar)\n","    return filtered_image"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JUV6rPC45YA6"},"outputs":[],"source":["# Define the function to apply the angle correction and convert to gamma0.\n","def to_gamma0(image):\n","    # Select the 'angle' band and apply the angle correction\n","    angle_rad = image.select('angle').multiply(ee.Number(math.pi).divide(180.0))\n","    cos_angle = angle_rad.cos()\n","    correction_factor = cos_angle.log10().multiply(10.0)\n","\n","    # Apply the correction to 'VV' band\n","    vv_corrected = image.select('VV').subtract(correction_factor)\n","    vh_corrected = image.select('VH').subtract(correction_factor)\n","\n","    # Return the image with corrected 'VV' band\n","    return image.addBands(vv_corrected.rename('VV_gamma0')).addBands(vh_corrected.rename('VH_gamma0'))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TIdhoi-s5qiv"},"outputs":[],"source":["# Function to compute the Sigma Lee filter\n","def sigma_lee(image, kernel_size=3, sigma=0.9):\n","    # Compute the local mean and variance\n","    reducer = ee.Reducer.mean().combine(\n","        reducer2=ee.Reducer.variance(),\n","        sharedInputs=True\n","    )\n","\n","    # Compute the mean and variance for each pixel within the window\n","    stats = image.reduceNeighborhood(\n","        reducer=reducer,\n","        kernel=ee.Kernel.square(kernel_size // 2),\n","        optimization='window'\n","    )\n","\n","    mean = stats.select(0)\n","    variance = stats.select(1)\n","\n","    # Compute the noise variance\n","    noise_variance = variance.sqrt().divide(mean).pow(2).multiply(sigma)\n","\n","    # Compute the coefficient of variation\n","    coef_variation = variance.sqrt().divide(mean)\n","\n","    # Compute the filtered value\n","    one = ee.Image.constant(1)\n","    filter_value = one.subtract(noise_variance.divide(variance)).multiply(image.subtract(mean)).add(mean)\n","\n","    # Mask invalid values\n","    filter_value = filter_value.updateMask(coef_variation.lte(sigma))\n","\n","    return filter_value"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iHW8PiSEAcui"},"outputs":[],"source":["# Function to extract spectral information at training points\n","def extract_data(feature):\n","    return feature.set(input.select(bands).reduceRegion(\n","        reducer=ee.Reducer.mean(),\n","        geometry=feature.geometry(),\n","        scale=10  # Spatial resolution in meters\n","    ))"]},{"cell_type":"markdown","metadata":{"id":"cLm0v0hZlAcj"},"source":["# **Get Sentinel images, filter by date and cloud cover and sample training data**"]},{"cell_type":"code","source":["# select only summer season for the Rocky Mountains, Alps and Himalayas and entire year for the Andes. The order is the same as in the training_list\n","start_date = ['2021-06-01','2021-06-01','2021-06-01','2021-01-01','2021-01-01','2021-01-01',\n","              '2021-01-01','2021-01-01','2021-06-01','2021-06-01','2021-01-01','2021-01-01']\n","end_date = ['2021-11-01','2021-11-01','2021-11-01','2021-12-31','2021-12-31','2021-12-31',\n","            '2021-12-31','2021-12-31','2021-11-01','2021-11-01','2021-12-31','2021-12-01']"],"metadata":{"id":"8R8eg1Kj9Z2h"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iwWQfsVH5ZMq"},"outputs":[],"source":["# Rasterize the 'ECO_ID' property into the image\n","eco_id_image = ecoregions.reduceToImage(\n","    properties=['ECO_ID'],\n","    reducer=ee.Reducer.first()\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b767a1Ue9_Lq"},"outputs":[],"source":["# create empty data frame\n","training_data = ee.FeatureCollection([])\n","\n","for training_region, hw, nhw, start, end in zip(training_list[:6], hw_list[:6], nhw_list[:6], start_date[:6], end_date[:6]): # select the first half of the data to avoid crashing\n","#for training_region, hw, nhw, start, end in zip(training_list[6:], hw_list[6:], nhw_list[6:], start_date[6:], end_date[6:]): # select the second half of the data to avoid crashing\n","#for training_region, hw, nhw, start, end in zip(training_list, hw_list, nhw_list, start_date, end_date): # select all data (not recommended!)\n","    mask = training_region # to cut data to each training region\n","    # get sentinel 2 data\n","    s2_image = ee.ImageCollection(\"COPERNICUS/S2_SR_HARMONIZED\") \\\n","                 .filterBounds(mask)\\\n","                 .filterDate(start, end)\\\n","                 .filter(ee.Filter.lte('CLOUDY_PIXEL_PERCENTAGE', 10))\\\n","                 .map(cloudless)\n","    s2_clipped = s2_image.map(lambda image: image.clip(mask))\n","    s2_m = s2_clipped.median()\n","\n","    # get sentinel 1 image\n","    s1_pol = ee.ImageCollection('COPERNICUS/S1_GRD')\\\n","               .filterBounds(mask)\\\n","               .filterDate(start, end)\\\n","               .filter(ee.Filter.listContains('transmitterReceiverPolarisation', 'VV'))\\\n","               .filter(ee.Filter.listContains('transmitterReceiverPolarisation', 'VH'))\\\n","               .filter(ee.Filter.eq('resolution_meters', 10))\n","    s1_pol = s1_pol.map(lambda image: image.clip(mask))\n","\n","    # Apply angle masking to s1_pol\n","    s1_pol1 = s1_pol.map(mask_ang_gt_30)\n","    s1_pol1 = s1_pol1.map(mask_ang_lt_45)\n","\n","    # Apply the windy day filter to the image collection\n","    #s1_pol1 = s1_pol1.map(pct_wat)\n","\n","    # Apply the function to the image collection\n","    NDPI = s1_pol1.map(calculate_gamma0_and_ndpi)\n","\n","    # Apply filter\n","    NDPI = NDPI.map(apply_filter)\n","\n","    # Get a median composite image to reduce noise and improve visibility\n","    NDPI = NDPI.median()\n","\n","    # mask out edges\n","    s1_pol2 = s1_pol.map(mask_ang_gt_30)\n","    s1_pol2 = s1_pol2.map(mask_ang_lt_45)\n","\n","    # apply angle correction\n","    s1_pol2 = s1_pol2.map(to_gamma0)\n","\n","    # Apply function for Sigma Lee speckle filtering\n","    s1_pol_lee_filtr = s1_pol2.map(sigma_lee)\n","    VV_VH = s1_pol_lee_filtr.mean()\n","    VVsd_VHsd = s1_pol_lee_filtr.reduce(ee.Reducer.stdDev())\n","\n","    # topographic position index (TPI)\n","    focal_mean = elevation_temp.clip(mask).focalMean(5)\n","    tpi = elevation_temp.clip(mask).subtract(focal_mean)\n","    tpi = tpi.rename(['tpi'])\n","\n","    # clip flow accumulation, slope and elevation\n","    elevation = elevation_temp.clip(mask)\n","    slope = slope_temp.clip(mask)\n","\n","    spec_indices = spectral(s2_m)\n","\n","    # ecoregions\n","    ecoregion = eco_id_image.clip(mask)\n","    ecoregion = ecoregion.rename(['ecoregion'])\n","\n","    indices = spec_indices.addBands(slope).addBands(elevation).addBands(tpi).addBands(VV_VH).addBands(VVsd_VHsd).addBands(NDPI).addBands(ecoregion)\n","\n","    # merge images with wetlands and w/o wetlands. This is the image the binary sample points are taken from (wetland yes/no).\n","    wetland_area = hw\n","    nonwetland_area = nhw\n","    wetland_area = wetland_area.map(lambda f: f.set('class', 1))\n","    nonwetland_area = nonwetland_area.map(lambda f: f.set('class', 0))\n","    training_area = wetland_area.merge(nonwetland_area)\n","\n","    # Reduce to image using the 'class' property\n","    raster = training_area.reduceToImage(\n","    properties=['class'],\n","    reducer=ee.Reducer.first()\n","    )\n","    raster = raster.rename('class')\n","\n","    # Optional: Set a specific scale (resolution) for the output raster\n","    sam_image = raster.reproject(crs='EPSG:4326', scale=10)\n","\n","    # Create the stratified sample (this computation takes time)\n","    training_points = sam_image.stratifiedSample(\n","        numPoints=3000,\n","        classBand='class',\n","        region=mask,\n","        scale=10,\n","        classValues=[0,1],\n","        classPoints=[1500,1500],\n","        geometries=True\n","    )\n","\n","    bands = ['ndvi', 'ndwi', 'elevation', 'tpi', 'slope', 'tcg', 'NDPI', 'VV_gamma0', 'VH_gamma0', 'reip', 'ari', 'ecoregion']\n","    input = indices.select(bands)\n","\n","    data = training_points.map(extract_data)\n","\n","    training_data = training_data.merge(data) # joins training data from all training_regions into one table to be used for training the classifier\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RXeFkescQSHA"},"outputs":[],"source":["# Filter the training data to remove features with null values in any of the properties\n","training_data = training_data.filter(ee.Filter.notNull(bands))"]},{"cell_type":"markdown","metadata":{"id":"-zS5XIW5LJga"},"source":["**Write out the training data**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"93zpsJvfLI8P"},"outputs":[],"source":["# export to drive (this mights take several hours!)\n","task = ee.batch.Export.table.toDrive(\n","    collection=training_data,\n","    description='training_data_1', # rename for the second half of the data (e.g. 'training_data_2')\n","    fileFormat='CSV',\n","    folder='earth_engine_exports',\n","    fileNamePrefix='training_data'\n",")\n","task.start()\n","\n","print(f'Started export task for training_data')"]},{"cell_type":"markdown","source":["As a final step for this part, load the exported csv files with the training data (training_data_1 and training_data_2) as assets to GEE."],"metadata":{"id":"yTK02hdnqo7P"}}],"metadata":{"colab":{"provenance":[{"file_id":"1kgEKvh320EZ-2c0PyblP-fhj0DenXFpZ","timestamp":1740565697477},{"file_id":"1O3pOX3ujKmO4wYUMwVXD7BlxZGmqtxWH","timestamp":1732097503627},{"file_id":"12ACoREAVoOZ3sqFOFSbfqG4y4LIdASWx","timestamp":1731237459198},{"file_id":"1xi3K7JiRz4oJ0y75XhzPFNksS8pORuS0","timestamp":1730885355520},{"file_id":"1tvblRmYGGfHmhftDDYeSpIWHn6F_ddoq","timestamp":1730794906911},{"file_id":"1jKDfyu5zkEuK9oMnglizwd-t2Vn6eghd","timestamp":1728892778629},{"file_id":"1_LxfO-af9vPHUoFJnaIgZDukX8QwEp-j","timestamp":1727444960339},{"file_id":"1P7CiLVtpjQcBb8RnI2KxKkgVZBkCaMoe","timestamp":1721980178643},{"file_id":"16P3b-sGZXzRkGwjgkOohmGmc8Sx3TAgU","timestamp":1721661163187}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}